import os
import numpy as np
import tensorflow as tf

from tqdm import tqdm
from tensorflow.keras import layers


class Logger():
    def __init__(self, log_dir):
        self.writer = tf.summary.create_file_writer(log_dir)
        
    def write(self, var_name, metric, step):
        with self.writer.as_default():
            tf.summary.scalar(var_name, metric.result(), step)
        metric.reset_states()


class NeuralNetwork(tf.keras.Model):
    def __init__(self, n_inputs, layer_sizes, n_outputs):
        super().__init__()

        nn_layers = tf.keras.Sequential()
        nn_layers.add(layers.Dense(n_inputs))
        
        for size in layer_sizes:
            nn_layers.add(layers.Dense(size))
            nn_layers.add(layers.LeakyReLU())
            
        nn_layers.add(layers.Dense(n_outputs))

        self.nn = nn_layers

    def call(self, inputs):
        return self.nn(inputs)


class Trainer:
    def __init__(self, world_model, robustness_computer, \
                attacker_nn, defender_nn, \
                attacker_loss_fn, defender_loss_fn, \
                attacker_optimizer, defender_optimizer, \
                working_dir, logging=False):

        self.model = world_model
        self.robustness_computer = robustness_computer

        self.attacker = attacker_nn
        self.defender = defender_nn
        self.attacker_loss_fn = attacker_loss_fn
        self.defender_loss_fn = defender_loss_fn
        self.attacker_optimizer = attacker_optimizer
        self.defender_optimizer = defender_optimizer

        self.checkpoint_path = os.path.join(working_dir, 'ckpt')
        self.checkpoint = tf.train.Checkpoint(attacker=attacker, defender=defender, \
                                            attacker_optimizer=attacker_optimizer, \
                                            defender_optimizer=defender_optimizer)

        self.logger = Logger(working_dir) if logging else None
        
        if self.logger:
            self.metric_loss = tf.keras.metrics.Mean()
            self.metric_rho = tf.keras.metrics.Mean()

    def train_step(self): #TODO
        return
        with tf.GradientTape() as tape:
            # Get current predictions of network
            y_pred = self.model(np.array([[0,0]]), training=True)
           
            # Calculate loss generated by predictions
            loss = self.loss_function(TARGET, y_pred)
            
            self.metric_loss.update_state(loss)
            self.metric_acc.update_state(TARGET, 0)

        gradients = tape.gradient(loss, self.model.trainable_variables)
        # Change trainable variable values according to gradient by applying optimizer policy
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        
        #self.train_loss(loss)

    def simulate(self, atk_output, def_output, start_from, simulation_horizon, dt):
        # project into the future (consider derivative in future)
        atk_constant = np.ones((simulation_horizon, len(atk_output)))
        atk_commands = atk_output * atk_constant
        def_constant = np.ones((simulation_horizon, len(def_output)))
        def_commands = def_output * def_constant

        _ = [self.model.step(atk_move, def_move, dt)
            for atk_move, def_move in zip(atk_commands, def_commands)]

        return self.robustness_computer.compute(self.model, start_from)

    def train(self, n_steps, n_episodes, simulation_horizon, dt):
        # generazione ambiente iniziale
        for i in range(n_steps):
            base_config = self.model.save()

            dataset = []

            for _ in range(n_episodes):
                # Tries to generate possible future moves
                atk_input = self.model.environment.get_status()
                def_input = self.model.agent.get_status()
                # adding noise to the input?
                # TODO
                # generating output from scratch?
                atk_output = self.attacker(atk_input)
                def_output = self.defender(def_input)

                # Evaluate the robustness of each choice
                rho = self.simulate(atk_output, def_output, i, \
                                    simulation_horizon, dt)

                dataset.append((atk_input, def_input, rho))

                self.model.restore(base_config)

            # Training on the dataset just built on episodes
            for atk_output, def_output, rho in dataset:
                self.train_step(atk_output, def_output, rho, i, \
                                simulation_horizon, dt)

            atk_input = self.model.environment.get_status()
            def_input = self.model.agent.get_status()

            atk_output = self.attacker(atk_input)
            def_output = self.defender(def_input)

            # Applies the choice on the physical model
            self.model.step(atk_output, def_output, dt)

    def test_step(self): #TODO
        self.test_loss(self.loss_function(y, self.model(x)))

    def test(self, n_steps): #TODO
        for _ in range(n_steps):
            self.test_step()

    def run(self, n_epochs, n_steps, n_episodes, simulation_horizon=100, dt=0.05):
        epochs = tqdm(range(n_epochs)) if self.logger else range(n_epochs)

        initial_config = self.model.save()

        for i in epochs:
            self.train(n_episodes, n_steps, simulation_horizon, dt)
            
            if (i + 1) % 10:
                self.checkpoint.save(file_prefix=self.checkpoint_path)
            
            if self.logger:
                pass #log rho

            self.model.restore(initial_config)
